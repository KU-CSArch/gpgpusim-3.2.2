# This file models the kepler architecture GTX 780 ti
# This file was created by Mahmoud Khairy (khairy2011@gmail.com)
# '##' refers to my commments 

# functional simulator specification
-gpgpu_ptx_instruction_classification 0
-gpgpu_ptx_sim_mode 0
-gpgpu_ptx_force_max_capability 20

# SASS execution (only supported with CUDA >= 4.0)
-gpgpu_ptx_convert_to_ptxplus 0
-gpgpu_ptx_save_converted_ptxplus 0

# high level architecture configuration
-gpgpu_n_clusters 16
-gpgpu_n_cores_per_cluster 1
-gpgpu_n_mem 6
-gpgpu_n_sub_partition_per_mchannel 2

## Kepler GTX 780 ti clock domains are adopted from 
## https://en.wikipedia.org/wiki/GeForce_700_series
-gpgpu_clock_domains 876.0:1752.0:876.0:1750.0

# shader core pipeline config
-gpgpu_shader_registers 65536

## This implies a maximum of 64 warps/SM
-gpgpu_shader_core_pipeline 2048:32 
-gpgpu_shader_cta 16
-gpgpu_simd_model 1 

# Pipeline widths and number of FUs
# ID_OC_SP,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_SFU,OC_EX_MEM,EX_WB
## Kepler GTX 780 ti has 6 SP SIMD units and 2 SFU units
## we also need to increase the number of pipeline registers to be equal to the number of SP units
-gpgpu_pipeline_widths 6,2,1,6,2,1,6
-gpgpu_num_sp_units 6
-gpgpu_num_sfu_units 2

# Instruction latencies and initiation intervals
# "ADD,MAX,MUL,MAD,DIV"
## we leave the same latencies as Fermi architecture
-ptx_opcode_latency_int 4,13,4,5,145
-ptx_opcode_initiation_int 1,1,1,1,1
-ptx_opcode_latency_fp 4,13,4,5,39
-ptx_opcode_initiation_fp 1,1,1,1,1
-ptx_opcode_latency_dp 8,19,8,8,330
-ptx_opcode_initiation_dp 1,1,1,1,1


## In Kepler, the cache and shared memory can be configured to 16kb:48kb or 48kb:16kb or 32kb:32kb
## conservatively, we set L1 and shared memory to 48 kb
# <nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:**<fifo_entry>
# ** Optional parameter - Required when mshr_type==Texture Fifo
#-gpgpu_cache:dl1  32:128:4,L:L:f:N:L,A:64:8,8
#-gpgpu_cache:dl1  64:128:6,L:L:m:N,A:64:8,8
-gpgpu_shmem_size 49152
-gpgpu_flush_l1_cache	1

## Kepler comes with 1.5 MB of L2 cache
## 128 sets, each 128 bytes 16-way for each memory partition. This gives 1.5MB L2 cache
## larger MSHRs entries are also required
-gpgpu_cache:dl2 64:128:16,L:B:m:W:L,A:128:8,4:0,32
#-gpgpu_cache:dl2 128:128:16,L:B:m:W,A:128:8,4
-gpgpu_cache:dl2_texture_only 0 

# 2 KB Inst.
-gpgpu_cache:il1 4:128:4,L:R:f:N:L,A:2:32,4
#-gpgpu_cache:il1 4:128:4,L:R:f:N,A:2:32,4
# 48 KB Tex (aka read-only data cache in kepler terminology)
-gpgpu_tex_cache:l1 16:128:24,L:R:m:N:L,F:128:4,128:2
#-gpgpu_tex_cache:l1 16:128:24,L:R:m:N,F:128:4,128:2
# 8 KB Const
-gpgpu_const_cache:l1 64:64:2,L:R:f:N:L,A:2:32,4
#-gpgpu_const_cache:l1 64:64:2,L:R:f:N,A:2:32,4

# enable operand collector 
## larger operand collectors nd reg_banks are needed for the 4 warp schedulers and 6 SIMD units
-gpgpu_operand_collector_num_units_sp 16
-gpgpu_operand_collector_num_units_sfu 4
-gpgpu_operand_collector_num_units_mem 4
-gpgpu_operand_collector_num_in_ports_sp 6
-gpgpu_operand_collector_num_out_ports_sp 6
-gpgpu_operand_collector_num_in_ports_sfu 2
-gpgpu_operand_collector_num_in_ports_sfu 2
-gpgpu_num_reg_banks 24

# shared memory bankconflict detection 
-gpgpu_shmem_num_banks 32
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 1

## In kepler, a warp scheduler can issue 2 insts per cycle
-gpgpu_max_insn_issue_per_warp 2

# interconnection
-network_mode 1 
#-inter_config_file config_kepler_islip.icnt

# memory partition latency config 
-rop_latency 120
-dram_latency 100

## larger dram queue size is also required
# dram model config
-gpgpu_dram_scheduler 1
-gpgpu_frfcfs_dram_sched_queue_size 64
-gpgpu_dram_return_queue_size 116
# gunjae: added more configurations for memory sub-partitions
-gpgpu_dram_partition_queues 8:8:8:8
-l2_ideal 0

# for kepler, bus width is 384bits, this is 8 bytes (4 bytes at each DRAM chip) per memory partition
-gpgpu_n_mem_per_ctrlr 2
-gpgpu_dram_buswidth 4
-gpgpu_dram_burst_length 8
-dram_data_command_freq_ratio 4  # GDDR5 is QDR
-gpgpu_mem_address_mask 1
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.BBBBCCCC.CCSSSSSS

# GDDR5 timing from hynix H5GQ1H24AFR
# tCK = clock period (1/1.75G)
# tCCDS = 2 tCK (diff bnkgrp)
# tCCDL = 3 tCK (same bnkgrp)
# tRRD = 5.5 ns * 1.75 = 9.625 (10 tCK)
# tRCD = 12 ns * 1.75 = 21 (22 tCK)
# tRAS = 28 ns * 1.75 = 49 (50 tCK)
# tRP = 12 ns * 1.75 = 21 (22 tCK)
# tRC = 40 ns * 1.75 = 70 (71 tCK)
# tWR = 12 ns * 1.75 = 21 (22 tCK)
# tCDLR = 5 ns * 1.75 = 8.75 (9 tCK, tWTRL in the data sheet)
# tRTPL = 2 tCK (diff bankgrp)
# CL = CAS latency (5 ~ 20 tCK)
# WL = WRITE latency (1 to 7 tCK)
# to disable bank groups, set nbkgrp to 1 and tCCDL and tRTPL to 0
# gunjae: need to recalculate
-gpgpu_dram_timing_opt "nbk=16:CCD=2:RRD=10:RCD=21:RAS=49:RP=21:RC=71:
                        CL=12:WL=4:CDLR=9:WR=22:nbkgrp=4:CCDL=3:RTPL=2"
#-gpgpu_dram_timing_opt "nbk=16:CCD=2:RRD=6:RCD=12:RAS=28:RP=12:RC=40:
#                        CL=12:WL=4:CDLR=5:WR=12:nbkgrp=4:CCDL=3:RTPL=2"

# Kepler has four schedulers per core
-gpgpu_num_sched_per_core 4
# Two Level Scheduler with active and pending pools
#-gpgpu_scheduler two_level_active:6:0:1
# Loose round robbin scheduler
-gpgpu_scheduler lrr
# Greedy then oldest scheduler
#-gpgpu_scheduler gto

# stat collection
-gpgpu_memlatency_stat 14 
-gpgpu_runtime_stat 500
-enable_ptx_file_line_stats 1
-visualizer_enabled 0

# power model configs
-power_simulation_enabled 0
-gpuwattch_xml_file gpuwattch_gtx480.xml

# tracing functionality
#-trace_enabled 1
#-trace_components WARP_SCHEDULER,SCOREBOARD
#-trace_sampling_core 0
